import torch
import torch.nn.functional as F
import cv2
import numpy as np
import os
import time
import sys

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
from pathlib import Path
from mmpose.apis import MMPoseInferencer
from mmaction.apis import init_recognizer
from mmengine.registry import init_default_scope
from tqdm import tqdm
from module.llm import get_ai_coach_feedback
from module.generate_prompt import generate_llm_prompt_en

# ==============================================================================
# 0. é—œç¯€é»åç¨±å®šç¾©
# ==============================================================================
KEYPOINT_NAMES = {
    0: "é¼»å­ (Nose)", 1: "å·¦çœ¼ (Left Eye)", 2: "å³çœ¼ (Right Eye)", 3: "å·¦è€³ (Left Ear)", 4: "å³è€³ (Right Ear)",
    5: "å·¦è‚© (Left Shoulder)", 6: "å³è‚© (Right Shoulder)", 7: "å·¦è‚˜ (Left Elbow)", 8: "å³è‚˜ (Right Elbow)",
    9: "å·¦è…• (Left Wrist)", 10: "å³è…• (Right Wrist)", 11: "å·¦é«– (Left Hip)", 12: "å³é«– (Right Hip)",
    13: "å·¦è† (Left Knee)", 14: "å³è† (Right Knee)", 15: "å·¦è¸ (Left Ankle)", 16: "å³è¸ (Right Ankle)"
}

# ==============================================================================
# 1. GradCAM æ ¸å¿ƒé¡åˆ¥ (å·²é©—è­‰)
# ==============================================================================
class STGCNGradCAM:
    def __init__(self, model, target_layer_name):
        self.model = model
        self.model.eval()
        self.target_layer_name = target_layer_name
        try:
            self.target_layer = self._get_target_layer()
        except Exception as e:
            print(f"éŒ¯èª¤: ç„¡æ³•æ‰¾åˆ°ç›®æ¨™å±¤ '{target_layer_name}'ã€‚è«‹æª¢æŸ¥æ¨¡å‹æ¶æ§‹ã€‚")
            sys.exit(1)
            
        self.feature_maps = {}
        self.gradients = {}
        self.handlers = []
        self._register_hooks()

    def _get_target_layer(self):
        module = self.model
        for name in self.target_layer_name.split('.'):
            module = module[int(name)] if name.isdigit() else getattr(module, name)
        return module

    def _register_hooks(self):
        def forward_hook(module, input, output):
            self.feature_maps[self.target_layer] = output.detach()
        def backward_hook(module, grad_in, grad_out):
            self.gradients[self.target_layer] = grad_out[0].detach()
        
        self.handlers.append(self.target_layer.register_forward_hook(forward_hook))
        self.handlers.append(self.target_layer.register_backward_hook(backward_hook))

    def _calculate_localization_map(self, feature_maps, grads):
        if grads is None or torch.all(grads == 0):
            print(f"\nè­¦å‘Š: ç›®æ¨™å±¤ '{self.target_layer_name}' çš„æ¢¯åº¦ç‚ºé›¶ï¼Œç„¡æ³•ç”Ÿæˆç†±åœ–ã€‚")
            return np.zeros(feature_maps.shape[2:], dtype=np.float32)

        weights = torch.mean(grads, dim=(2, 3), keepdim=True)
        cam = torch.sum(feature_maps * weights, dim=1)
        cam = F.relu(cam)
        return cam.cpu().numpy()

    def __call__(self, inputs, index=-1):
        self.model.zero_grad()
        
        from mmengine.dataset import Compose, pseudo_collate
        from mmengine.registry import DefaultScope

        cfg = self.model.cfg
        test_pipeline_cfg = cfg.get('test_pipeline', cfg.get('val_pipeline'))

        with DefaultScope.overwrite_default_scope('mmaction'):
            test_pipeline = Compose(test_pipeline_cfg)
        
        data = test_pipeline(inputs.copy())
        data = pseudo_collate([data])
        
        with torch.no_grad():
            results_for_pred = self.model.test_step(data)[0]
        
        scores = results_for_pred.pred_score
        if index == -1:
            index = scores.argmax().item()
        pred_score = scores[index].item()

        results_for_grad = self.model.test_step(data)[0]
        score_for_backward = results_for_grad.pred_score[index]
        
        score_for_backward.backward()
        
        feature_maps = self.feature_maps[self.target_layer]
        grads = self.gradients[self.target_layer]
        localization_map = self._calculate_localization_map(feature_maps, grads)
        
        final_map = localization_map[0] if len(localization_map.shape) > 2 else localization_map
        
        return final_map, index, pred_score

    def remove_hooks(self):
        for handle in self.handlers:
            handle.remove()

# ==============================================================================
# 2. è¦–è¦ºåŒ–è¼”åŠ©å‡½å¼ (å·²é©—è­‰)
# ==============================================================================
coco_skeleton = [[15, 13], [13, 11], [16, 14], [14, 12], [11, 12],
                 [5, 11], [6, 12], [5, 6], [5, 7], [6, 8], [7, 9],
                 [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4],
                 [3, 5], [4, 6]]

def visualize_gradcam_on_frame(frame, keypoints, gradcam_scores, skeleton_conn, frame_size):
    fig, ax = plt.subplots(figsize=(frame_size[0] / 100, frame_size[1] / 100), dpi=100)
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
    ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    ax.axis('off')
    cmap = plt.get_cmap('jet')
    colors = np.array([[0.2, 0.2, 1.]] * len(keypoints))
    if gradcam_scores is not None and np.any(gradcam_scores):
        enhanced_scores = np.nan_to_num(gradcam_scores**0.5)
        colors = cmap(enhanced_scores)[:, :3]
    ax.scatter(keypoints[:, 0], keypoints[:, 1], c=colors, s=150, zorder=2, alpha=0.9)
    lines, line_colors = [], []
    for p1_idx, p2_idx in skeleton_conn:
        if p1_idx < len(keypoints) and p2_idx < len(keypoints) and \
           keypoints[p1_idx, 0] > 0 and keypoints[p2_idx, 0] > 0:
            lines.append([keypoints[p1_idx], keypoints[p2_idx]])
            line_colors.append((colors[p1_idx] + colors[p2_idx]) / 2)
    lc = LineCollection(lines, colors=line_colors, linewidths=8, zorder=1, alpha=0.85)
    ax.add_collection(lc)
    fig.canvas.draw()
    
    buf = fig.canvas.tostring_argb()
    img_argb = np.frombuffer(buf, dtype=np.uint8).reshape(fig.canvas.get_width_height()[::-1] + (4,))
    img_rgba = np.roll(img_argb, 3, axis=2)
    img_rgb = img_rgba[:, :, :3]
    
    plt.close(fig)
    return img_rgb

# ==============================================================================
# 3. ä¸»å‡½å¼ (æœ€çµ‚ä¿®æ­£ç‰ˆ)
# ==============================================================================
def main():
    total_start_time = time.time()

    init_default_scope('mmaction')
    os.makedirs("gradcam_video_output/", exist_ok=True)
    os.makedirs("gradcam_text_output/", exist_ok=True)
    # ACTION_REC_CONFIG = '/home/cvlab123/mmaction2/configs/skeleton/custom_stgcnpp_test/stgcnpp_8xb16-bone-motion-u100-80e_OurDataset-xsub-keypoint-2d.py'
    # # ACTION_REC_CONFIG = '/home/cvlab123/mmaction2/configs/skeleton/custom_stgcnpp/stgcnpp_8xb16-bone-u100-80e_OurDataset-xsub-keypoint-2d.py'
    # # ACTION_REC_CONFIG = '/home/cvlab123/mmaction2/configs/skeleton/custom_stgcnpp_test/stgcnpp_8xb16-joint-motion-u100-80e_OurDataset-xsub-keypoint-2d.py'
    # # ACTION_REC_CONFIG = '/home/cvlab123/mmaction2/configs/skeleton/custom_stgcnpp_test/stgcnpp_8xb16-joint-u100-80e_OurDataset-xsub-keypoint-2d.py'
    
    
    # ACTION_REC_CHECKPOINT = '/home/cvlab123/mmaction2/work_dirs/916_rtmpose_all_2D_bone_motion/best_acc_top1_epoch_16.pth'
    # # ACTION_REC_CHECKPOINT = '/home/cvlab123/mmaction2/work_dirs/916_rtmpose_all_2D_bone/best_acc_top1_epoch_9.pth'
    # # ACTION_REC_CHECKPOINT = '/home/cvlab123/mmaction2/work_dirs/916_rtmpose_all_2D_joint_motion/best_acc_top1_epoch_10.pth'
    # # ACTION_REC_CHECKPOINT = '/home/cvlab123/mmaction2/work_dirs/916_rtmpose_all_2D_joint/best_acc_top1_epoch_13.pth'
    # # --- Aligning with demo.py ---
    # Pose Estimation Model (RTMPose-S)
    POSE_CONFIG = '/home/cvlab123/mmpose/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py'
    POSE_CHECKPOINT = '/home/cvlab123/mmpose/checkpoints/rtmpose-s_simcc-aic-coco_pt-aic-coco_420e-256x192-fcb2599b_20230126.pth'

    # # POSE_CONFIG = '/home/cvlab123/mmpose/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py'
    # POSE_CONFIG = '/home/cvlab123/mmpose/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py'
    # # POSE_CHECKPOINT = '/home/cvlab123/mmpose/checkpoints/rtmpose-s_simcc-aic-coco_pt-aic-coco_420e-256x192-fcb2599b_20230126.pth'
    # POSE_CHECKPOINT = '/home/cvlab123/mmpose/checkpoints/rtmpose-l_simcc-aic-coco_pt-aic-coco_420e-256x192-f016ffe0_20230126.pth'
    # Action Recognition Model (STGCN++ Joint)
    ACTION_REC_CONFIG = '/home/cvlab123/mmaction2/configs/skeleton/custom_stgcnpp/stgcnpp_8xb16-joint-u100-80e_OurDataset-xsub-keypoint-2d.py'
    ACTION_REC_CHECKPOINT = '/home/cvlab123/mmaction2/work_dirs/916_rtmpose_all_2D_joint/best_acc_top1_epoch_13.pth'
    # --- End of Alignment ---
    
    VIDEO_PATH = '/home/cvlab123/data/test_data/20250627_172807.mp4'
    
    video_stem = Path(VIDEO_PATH).stem
    OUTPUT_VIDEO_DIR = Path("gradcam_video_output")
    OUTPUT_VIDEO_PATH = OUTPUT_VIDEO_DIR / f"{video_stem}_visualize.mp4"
    OUTPUT_TEXT_DIR = Path("gradcam_text_output")
    OUTPUT_TEXT_PATH = OUTPUT_TEXT_DIR / f"{video_stem}_llm_text.txt"
    
    WINDOW_SIZE = 40
    STRIDE = 1
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    start_time = time.time()
    print("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
    action_model = init_recognizer(ACTION_REC_CONFIG, ACTION_REC_CHECKPOINT, device=device)
    pose_inferencer = MMPoseInferencer(pose2d=POSE_CONFIG, pose2d_weights=POSE_CHECKPOINT, device=device)
    print("æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    print(f"--- â±ï¸  æ¨¡å‹è¼‰å…¥è€—æ™‚: {time.time() - start_time:.2f} ç§’ ---\n")
    
    # [FIX] Use a list for labels to guarantee order. The index is the class ID.
    idx_to_label = [
        "lunge_correct", "lunge_knee_pass_toe", "lunge_too_high",
        "push_up_arched_back", "push_up_correct", "push_up_elbow",
        "squat_correct", "squat_feet_too_close", "squat_knees_inward"
    ]

    start_time = time.time()
    print(f"æ­£åœ¨å¾å½±ç‰‡ {VIDEO_PATH} æå–éª¨æ¶...")
    cap = cv2.VideoCapture(VIDEO_PATH)
    video_frames = [frame for success, frame in iter(lambda: cap.read(), (False, None))]
    cap.release()
    
    # [RECOMMENDED FIX] Add this line to match your training data sampling
    print(f"Original frame count: {len(video_frames)}. Downsampling by skipping every 2nd frame...")
    video_frames = video_frames[::2]
    print(f"New frame count after downsampling: {len(video_frames)}.")
    
    if not video_frames:
        print(f"éŒ¯èª¤ï¼šç„¡æ³•è®€å–å½±ç‰‡ '{VIDEO_PATH}' æˆ–å½±ç‰‡ç‚ºç©ºã€‚")
        return

    frame_h, frame_w, _ = video_frames[0].shape
    results_generator = pose_inferencer(video_frames, show_progress=True, batch_size=16)
    
    keypoints_list = [
        np.hstack([p['predictions'][0][0]['keypoints'], np.array(p['predictions'][0][0]['keypoint_scores'])[:, None]])
        if p['predictions'] and p['predictions'][0] else np.zeros((17, 3))
        for p in results_generator
    ]
    total_frames = len(keypoints_list)
    print(f"éª¨æ¶æå–å®Œæˆï¼Œå…± {total_frames} å¹€ã€‚")    
    print(f"--- â±ï¸  å½±ç‰‡è®€å–èˆ‡éª¨æ¶æå–è€—æ™‚: {time.time() - start_time:.2f} ç§’ ---\n")

    start_time = time.time()
    print("æ­£åœ¨ä½¿ç”¨æ»‘å‹•çª—å£è¨ˆç®— Grad-CAM...")
    target_layer_name = 'backbone.gcn.9.gcn'
    gradcam = STGCNGradCAM(action_model, target_layer_name)
    
    full_gradcam_map = np.zeros((total_frames, 17))
    overlap_counter = np.zeros((total_frames, 17))
    
    overall_pred_name = "N/A"
    overall_pred_score = 0.0

    if total_frames < WINDOW_SIZE:
        print(f"éŒ¯èª¤: å½±ç‰‡ç¸½å¹€æ•¸ ({total_frames}) å°æ–¼çª—å£å¤§å° ({WINDOW_SIZE})ã€‚ç„¡æ³•è™•ç†ã€‚")
        return
    
    # ç´€éŒ„æ¯å€‹ class çš„ score
    class_score_sum = {}
    class_pred_count = {}

    for i in tqdm(range(0, total_frames - WINDOW_SIZE + 1, STRIDE), desc="æ»‘å‹•çª—å£é€²åº¦"):
        start_frame = i
        end_frame = i + WINDOW_SIZE
        window_keypoints = np.array(keypoints_list[start_frame:end_frame])
        
        # [FIX] Split keypoints and scores to match the training pipeline format
        keypoints_for_anno = window_keypoints[..., :2]  # Shape: (WINDOW_SIZE, 17, 2)
        scores_for_anno = window_keypoints[..., 2]    # Shape: (WINDOW_SIZE, 17)

        anno = {
            'keypoint': keypoints_for_anno[np.newaxis, ...],
            'keypoint_score': scores_for_anno[np.newaxis, ...],
            'total_frames': WINDOW_SIZE,
            'img_shape': (frame_h, frame_w)
        }
        try:
            gradcam_map, pred_idx, pred_score = gradcam(anno)
            
            # [æœ€çµ‚ä¿®æ­£] å°‡ gradcam_map (ä¾‹å¦‚ shape 10,17) æ”¾å¤§å›çª—å£å¤§å° (48,17)
            # cv2.resize çš„åƒæ•¸ dsize æ˜¯ (å¯¬, é«˜)ï¼Œå°æ‡‰æˆ‘å€‘çš„ç¶­åº¦æ˜¯ (V, T)
            target_shape = (17, WINDOW_SIZE) # (V, T)
            resized_gradcam_map = cv2.resize(gradcam_map, target_shape, interpolation=cv2.INTER_LINEAR)
            
            # resized_gradcam_map çš„ shape æ˜¯ (48, 17)ï¼Œå¯ä»¥ç›´æ¥èˆ‡åˆ‡ç‰‡ç›¸åŠ 
            full_gradcam_map[start_frame:end_frame] += resized_gradcam_map
            overlap_counter[start_frame:end_frame] += 1

            pred_name = idx_to_label[pred_idx] if 0 <= pred_idx < len(idx_to_label) else "Unknown"
            class_score_sum[pred_name] = class_score_sum.get(pred_name, 0) + pred_score
            class_pred_count[pred_name] = class_pred_count.get(pred_name, 0) + 1

        except Exception as e:
            print(f"è™•ç†çª—å£ {start_frame}-{end_frame} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue

    gradcam.remove_hooks()
    
    # After the loop and gradcam.remove_hooks()
    if class_score_sum:
        # --- Step 1: Calculate the average score for each class ---
        class_avg_scores = {
            name: class_score_sum[name] / class_pred_count[name] 
            for name in class_score_sum
        }
        
        # --- Step 2: Find the class with the highest average score ---
        best_pred_name = max(class_avg_scores, key=class_avg_scores.get)
        best_avg_score = class_avg_scores[best_pred_name]

        print(f"Aggregated average predictions: {class_avg_scores}")
        
        # --- Step 3: Apply the 0.5 threshold ---
        if best_avg_score >= 0.5:
            overall_pred_name = best_pred_name
            # IMPORTANT: Also update the score to be the more meaningful average score
            overall_pred_score = best_avg_score 
            print(f"Final Prediction: '{overall_pred_name}' (Avg Score: {overall_pred_score:.4f})")
        else:
            overall_pred_name = "Not Detected"
            overall_pred_score = best_avg_score # Still useful to know the score
            print(f"Best prediction '{best_pred_name}' score ({best_avg_score:.4f}) is below threshold.")

    else:
        # This handles the case where no predictions were made at all
        overall_pred_name = "Not Detected"
        overall_pred_score = 0.0
        print("No valid predictions were made across any window.")
        
    overlap_counter[overlap_counter == 0] = 1
    final_gradcam_map = full_gradcam_map / overlap_counter
    print(f"æ¨¡å‹é æ¸¬çµæœ (ä»£è¡¨): {overall_pred_name} (åˆ†æ•¸: {overall_pred_score:.4f})")
    print(f"--- â±ï¸  Grad-CAM è¨ˆç®—è€—æ™‚: {time.time() - start_time:.2f} ç§’ ---\n")

    start_time = time.time()
    print("æ­£åœ¨ç”Ÿæˆè¦–è¦ºåŒ–å½±ç‰‡...")
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, 25, (frame_w, frame_h))

    for frame_idx, frame in enumerate(tqdm(video_frames, desc="ç”Ÿæˆå½±ç‰‡")):
        kps = keypoints_list[frame_idx][:, :2]
        
        frame_scores = final_gradcam_map[frame_idx]
        scores_for_frame = None
        if frame_scores.max() > frame_scores.min():
            scores_for_frame = (frame_scores - frame_scores.min()) / (frame_scores.max() - frame_scores.min() + 1e-6)
        
        vis_frame_rgb = visualize_gradcam_on_frame(frame, kps, scores_for_frame, coco_skeleton, (frame_w, frame_h))
        vis_frame_bgr = cv2.cvtColor(vis_frame_rgb, cv2.COLOR_RGB2BGR)

        text = f'Pred: {overall_pred_name} ({overall_pred_score:.2f})'
        cv2.putText(vis_frame_bgr, text, (20, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA)
        video_writer.write(vis_frame_bgr)
        
    video_writer.release()
    print(f"âœ… Grad-CAM è¦–è¦ºåŒ–å½±ç‰‡å·²å„²å­˜è‡³: {OUTPUT_VIDEO_PATH}")
    print(f"--- â±ï¸  è¦–è¦ºåŒ–å½±ç‰‡ç”Ÿæˆè€—æ™‚: {time.time() - start_time:.2f} ç§’ ---\n")

    # --- [æ–°å¢] ç”Ÿæˆä¸¦å°å‡º LLM Prompt ---
    start_time = time.time()
    print("\n" + "="*80)
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆçµ¦ LLM çš„ Prompt...")
    print("="*80)
    
    llm_prompt = generate_llm_prompt_en(
        final_gradcam_map, 
        overall_pred_name, 
        overall_pred_score, 
        KEYPOINT_NAMES, 
        total_frames
    )
    print(llm_prompt)
    print("="*80)
    print(f"--- â±ï¸  LLM Prompt ç”Ÿæˆè€—æ™‚: {time.time() - start_time:.2f} ç§’ ---")
    

    print(f"âœ… Prompt has been saved to {OUTPUT_TEXT_PATH}")

    start_time = time.time()
    llm_response = get_ai_coach_feedback(llm_prompt)
    print(f"----- AI Coach Feedback: -----")
    print(llm_response)
    print(f"--- â±ï¸  LLM æ¨ç†è€—æ™‚: {time.time() - start_time:.2f} ç§’ ---\n")

    print("="*80)
    print(f"ğŸ‰ å…¨éƒ¨è™•ç†å®Œæˆï¼ç¸½è€—æ™‚: {time.time() - total_start_time:.2f} ç§’")
    # Save to file
    with open(OUTPUT_TEXT_PATH, "w", encoding="utf-8") as f:
        f.write(llm_prompt)
        f.write(llm_response)
    
if __name__ == '__main__':
    # if not all(os.path.exists(f) for f in ['stgcnpp_8xb16-bone-motion-u100-80e_OurDataset-xsub-keypoint-2d.py', 'best_acc_top1_epoch_11.pth']):
    #     print("éŒ¯èª¤: è«‹ç¢ºä¿è¨­å®šæª”å’Œæ¬Šé‡æª”èˆ‡æœ¬è…³æœ¬åœ¨åŒä¸€ç›®éŒ„ä¸‹ã€‚")
    #     sys.exit(1)
    main()